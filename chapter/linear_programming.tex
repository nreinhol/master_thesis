\section{Linear Programming}
\label{sec:linear_progamming}
This section gives an introduction into the basics of mathematical linear programming, 
which forms the basis for the BTM applied in the BLEMS.
It presents the purpose and necessity of optimization models, gives
an example of how an optimization model is constructed and explains the duality theory,
which is essential for determining the market prices in the BTM and for the developed 
agent trade verification procedure, outlined later on in \ref{sec:agent_class}. Finally,
the used algorithm to solve such optimization models is presented.

Optimization models mathematically define the goal of solving a problem in the optimal way.
This can be applied in many different areas, for example, it might mean running a business to 
maximize profit, designing a bridge to minimize weight or selecting a flight plan for an aircraft
to minimize time or fuel use \shortcite{griva2009linear}. 
The use case of solving an optimization problem optimally is so ubiquitous, 
that optimization models are used in almost every area of application \shortcite{griva2009linear}.
Often it is not possible or economically feasible to make decisions without the help
of such a model. Due to the excellent improvements in computer hardware and software
in the last decades, optimization models became a practical tool 
in business, science and engineering \shortcite{griva2009linear}. 
Finally, it is possible to solve optimization problems with a huge set of variables. 

Linear optimization, also known as a \textit{linear program (LP)}, refers to the optimization
of a linear function with the decision variables, $x_{1}, ..., x_{n}$ subject to 
linear equality or inequality constraints. 
As presented by \shortciteA{bichler2017market}, in canonical form 
a linear optimization model is given as:

\begin{equation*}
    \begin{array}{ll@{}ll}
        \text{min}  & \displaystyle\sum\limits_{j=1}^{n} c_{j}x_{j} &\\
        \text{s.t.}& \displaystyle\sum\limits_{j=1}^{n} a_{ij}x_{j} \geq b_{i},  &&i=1 ,..., m\\
                    &                        x_{j} \geq 0, &&j=1 ,..., n
    \end{array}
\end{equation*}

However, a LP can be modeled in different forms. It is also possible 
to write a model in the matrix-vector notation. To represent the above model in this form, letting
$x=(x_{1}, ..., x_{n})^{T}$, $c=(c_{1}, ..., c_{n})^{T}$, $b=(b_{1}, ..., b_{m})^{T}$
and name the matrix of the coefficients $a_{ij}$ by $A$. As introduced by \shortciteA{griva2009linear},
the model becomes:

\begin{equation*}
    \begin{array}{ll@{}ll}
        \text{min}  & \displaystyle c^{T}x &\\
        \text{s.t.}& \displaystyle Ax \geq b&\\
                    &                        x \geq 0
    \end{array}
\end{equation*}

\clearpage
In general, the objective function of a linear model is either minimized or maximized.
Additionally, the constraints may include a combination of inequalities and equalities
and the variables are unrestricted or restricted in sign \shortcite{bichler2017market}.

\subsection{Duality Theory}
\label{sec:duality_theory}
For every LP exists another problem, which
is called the \textit{dual} of the original linear problem, also known as the 
\textit{primal} \shortcite{bichler2017market}.
In the dual, the roles of variables and constraints are reversed. 
That means, every variable of the primal becomes a constraint in the dual,
and every constraint in the primal becomes a variable in 
the dual \shortcite{griva2009linear}.
For instance, if the primal has $n$ variables and $m$ constraints, the dual
will have $m$ variables and $n$ constraints. Further, the primal objective coefficients 
are the coefficients on the right-hand side of 
the dual, and vice versa.
Finally, the transposed coefficient matrix of the primal constitutes the matrix in the 
dual \shortcite{griva2009linear}.
To give an example of a primal and the corresponding dual LP,
the representation by \shortciteA{bichler2017market} is considered:

\begin{equation}
    \tag{primal}
    \begin{array}{ll@{}ll}
        \text{min}  & \displaystyle c^{T}x &\\
        \text{s.t.}& \displaystyle Ax \geq b  &\\
                    &                        x \geq 0
    \end{array}
\end{equation}

\begin{equation}
    \tag{dual}
    \begin{array}{ll@{}ll}
        \text{max}  & \displaystyle b^{T}y &\\
        \text{s.t.}& \displaystyle A^{T}y \leq c &\\
                    &                        y \geq 0
    \end{array}
\end{equation}


Next, the motivation of the duality theory can be explained by the issue of trying to find 
bounds on the value of the optimal solution to a LP. 
If the primal is a minimization problem, the dual represents a lower bound
on the value of the optimal solution, otherwise, if the primal is a maximization problem,
the dual represents an upper bound \shortcite{bichler2017market}.
To illustrate the concept and motivation of duality theory more tangibly, an example stated by 
\shortciteA{bichler2017market} is given. 
In an application, the variables in the primal linear problem stand for consumer products and
the objective coefficients represent the profits linked to the production of those consumer products. 
In this case, the objective in the primal describes directly the relation between a change
in the production of the products and profit. Whereas, the constraints in the primal 
describe the availability of raw materials. Consequently, an increase in the availability of 
raw materials needed for the production of the consumer products enables an increase in production,
and finally an increase in the overall profit. 
However, the primal problem does not outline this relationship reasonable. Hence, one of 
the benefits of the dual is to properly show the effect of changes in the constraints on the
value of the objective. Due to this, the variables in a dual LP often called 
\textit{shadow prices}, since they describe the hidden costs associated with the constraints. 

\subsubsection{Weak Duality Theorem}
As stated in the section before (\ref{sec:duality_theory}), the \textit{dual} problem 
provides, depending on maximization or minimization problem, lower and upper bounds for 
the \textit{primal} objective function. Referring to \shortciteA{vanderbei2015linear},
this result is true in general and is described as the \textit{Weak Duality Theorem}:

\begin{theorem}
    If $x$ is feasible for the primal and 
    $y$ is feasible for the dual, then:

    \begin{equation*}
        \begin{array}{ll@{}ll}
            \displaystyle c^{T}x \leq b^{T}y&\\
        \end{array}
    \end{equation*}    

\end{theorem}

Regarding to \shortciteA{griva2009linear}, there are some logical corollaries 
of the Weak Duality Theorem:

\begin{corollary}
    If the primal is unbounded, then the dual is infeasible. 
    If the dual is unbounded, then the primal is infeasible.
\end{corollary}

\begin{corollary}
\label{strong_duality_corollary}
    If $x$ is a feasible solution to the primal, 
    $y$ is a feasible solution to the dual,
    and $c^{T}x = b^{T}y$, then x and y are optimal for their respective problems.
\end{corollary}

\subsubsection{Strong Duality Theorem}
\label{sec:strong_duality_theorem}
Concerning corollary \ref{strong_duality_corollary}, it is possible to verify if 
the points x and y are optimal solutions, without solving the corresponding LPs. 
Furthermore, this corollary is also used in the proof of strong duality \shortcite{griva2009linear}. 
The \textit{Strong Duality Theorem} describes, that for linear programming there 
is never a gap between the primal and the dual 
optimal objective values \shortcite{vanderbei2015linear}.

\begin{theorem}
    If  the primal problem has an optimal solution $x^{*}$,
    then the dual also has an optimal solution $y^{*}$,
    such that:

    \begin{equation*}
        \begin{array}{ll@{}ll}
            \displaystyle c^{T}x^{*} = b^{T}y^{*}&\\
        \end{array}
    \end{equation*}    

\end{theorem}

\subsubsection{Simplex-Method}
This subsection will give a brief introduction to the simplex-method.
In the field of linear programming, the simplex-method 
is the most widely used algorithm due to its  efficiency \shortcite{griva2009linear}.
In general, the simplex-method is an iterative algorithm, used to solve 
a LP \shortcite{griva2009linear}. To apply the algorithm to a LP, 
it must be in standard form \shortcite{Luenberger2008}. According to \shortciteA{griva2009linear}, 
we consider the following LP:

\begin{equation*}
    \begin{array}{ll@{}ll}
        \text{min}  & \displaystyle z = -x_{1} - 2x_{2} &\\
        \text{s.t.}& \displaystyle - 2x_{1} + x_{2} \leq 2 &\\
                    & \displaystyle - x_{1} + 2x_{2} \leq 7 &\\
                    & \displaystyle  x_{1} \leq 3 &\\
                    &                        x_{1}, x_{2} \geq 0
    \end{array}
\end{equation*}

Next, we give an example to illustrate how to convert a LP into standard form.
To achieve that, the so-called \textit{slack variables} will be added, which represents the 
difference between the right-hand side and the left-hand side. 

\begin{equation*}
    \begin{array}{ll@{}ll}
        \text{min}  & \displaystyle z = -x_{1} - 2x_{2} &\\
        \text{s.t.}& \displaystyle - 2x_{1} + x_{2} + x_{3} = 2 &\\
                    & \displaystyle - x_{1} + 2x_{2} + x_{4} = 7 &\\
                    & \displaystyle  x_{1} + x_{5} = 3 &\\
                    &                        x_{1}, x_{2}, + x_{3}, + x_{4}, + x_{5} \geq 0
    \end{array}
\end{equation*}


Finally, with the LP in standard form, the idea of the simplex-method
is to iteratively proceed from one basic feasible solution to another \shortcite{Luenberger2008}.
Thereby, it always looks for a feasible solution which is better than the one before. 
The definition of better, in this case, depends on the nature of the linear problem. 
The process is performed until a solution is reached, which cannot 
be improved anymore. In the end, this final solution is then an optimal solution \shortcite{vanderbei2015linear}. 


\clearpage




